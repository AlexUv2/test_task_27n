# Test Task 27n

### Задача 1
*Запропонувати методи визначення стану гри на стрімі або скріншоті. Розглядаються стріми тривалістю кілька годин з можливими перервами між іграми, відображенням інтерфейсу лобі, відео з веб-камери, реклами тощо. Метою є розробка рішення, яке забезпечить швидке та ефективне визначення того, чи відбувається гра на даний момент чи ні.*

Приклади рішення:

1. Classification
2. Activity Recognition 
3. Optical Flow
4. Object Detection

Найочевиднішим рішенням даної задачі є використання звичайної **Classification** - це є найпростішим способом, який можна реалізувати маючи досить невелику кількість даних і підібравши правильні аугментації, також ми не потребуватимемо кожен кадр стріма для цього, в залежності від того як часто ми хочемо знати стан стріма - нам може бути достатньо навіть 1-2FPS. Маючи, умовно, 5 класів (гра, камера стімера, реклама, лобі, пауза) цей спосіб повинен показувати себе досить гарно. Перелік моделей та архітектур для цього метода є досить широким - від класичних **VGG**, **ResNet**, **EfficientNet**, **MobileNet** до більш сучасних **YOLOV8**, **ViT**, **DeiT**, **Swin**, **RegNet** etc. Обирати архітектуру треба відповідно до результатів які ми хочему отримати, кількості даних та обчислювальних потежностей.

Переваги: швидкість, простота, можливість використання на мобільних пристроях.

Недоліки: при інференсі на низькому FPS можемо втратити деяку інформацію, для досягнення кращих результатів потрібно більше даних.


Другим способом є **Activity Recognition**. Цей варіан є вже більш затратним в обчисленнях, адже потрубує більної кількості кадрів зі стріма для опрацювання, що призведе до більшого часу інференсу. Для цього ми можемо використовувати той самий підхід класифікації, тобто ми матимемо предікт для кожного фрейма, але тут може вилізти проблема фолсів, тому з ними нам може допомогти боротися алгоритм по типу Калман-фільтра, який згладжуватиме аутлаєри в предіктах.

Також нам гарно підійде варіант використанн 3D CNN архітектур, які можуть використовувати інформацію про сусідні кадри, що дозволить нам більш точно визначити стан стріму.

Обидва підходи є базовими і використовуються досить часто для подібних задач. 

Переваги: можемо точно знати стан стріма в кожний момент часу, це має покращити загальну точність.
Недоліки: час інференсу може зрости через велику кількість даних для інференса в конкретний момент часу, але це може бути вирішено шляхом зміни архітектури.

**Optical Flow** - в наш час вже не такий популярних підхід, але завдяки [FlowNet2.0](https://arxiv.org/pdf/1612.01925.pdf) він став досить точним і швидким. Цей підхід може використовуватися для визначення руху об'єктів на кадрі, а також для визначення руху між кадрами. Це може бути дуже корисно для нашої задачі, оскільки ми можемо використовувати цю інформацію для визначення стану стріму. Наприклад, якщо ми визначили, що на кадрі є рух, але він не відповідає руху гравця, то ми можемо визначити, що на цьому кадрі відбувається реклама, або інша неігрова активність. Цей підхід може бути дуже корисним для визначення стану стріму, але він не дуже точний, тому для визначення стану стріму він може бути використаний лише як допоміжний інструмент.

Переваги: можемо визначити стан стріма в кожний момент часу, можемо використовувати інформацію про рух між кадрами, також буде зручним щоб визначати саме моменти зміни стану (тобто переходу з одного стану в інший).

Недоліки: не дуже точний, є досить застарілим підходом.


**Object Detection** також може бути гарним варіантом для даної проблеми, ми можемо натренувати модель, умовно, на декілька класів, яка буде детектити реальну людину, персонажів ігор, і певні обʼєкти реклами(логотипи, бренди, тощо). Це може бути корисно для визначення стану стріму, оскільки ми можемо визначити, що на кадрі є реальна людина, але вона не відповідає гравцю, тоді ми можемо визначити, що на цьому кадрі відбувається реклама, або інша неігрова активність. Цей підхід може бути дуже корисним для визначення стану стріму, але він не дуже точний, тому для визначення стану стріму він може бути використаний як допоміжний інструмент, або ж як основний, якщо ми зможемо натренувати модель достатньо точно при великій кільокості даних.

Переваги: може досить точно визначати певні обʼєкти на кадрі, можемо використовувати інформацію про рух між кадрами, також буде зручним щоб визначати саме моменти зміни стану (тобто переходу з одного стану в інший).

Недоліки: потребує набагато більної кількості різноманітних даних для тренування.


Вибір підходу для даної задачі має спиратись на постановку задачі, кількість даних, та результат який ми хочему отримат. Можна також додавати досить багато логіки, такої як, наприклад, *regions of interest*, *сombination with object detectors*, але це вже буде вже більш специфічно для конкретної задачі.

З готових рішень для action recognition та action classification зараз є [MMAction](https://github.com/open-mmlab/mmaction2) та [PyTorchVideo](https://github.com/facebookresearch/pytorchvideo), так ці підходи більше заточені активність людей, але за допомогою fine-nuting можна натренувати їх для даної задачі.

**Висновок:** Для даної задачі визначення стану стріма, найкращими підходами будуть **Classification** та **Action Recognition**, так як вони є більш "легкими" в плані збору даних, тренування, та логіки в цілому. В свою ж чергу **Optical Flow** та **Object Detection** можуть використовуватись для визначення стану стріма як допоміжні інструменти, або ж як основний, якщо ми зможемо натренувати модель достатньо точно при великій кільокості даних. Як і у будь який DL задачі потрібно проводити експермиенти та досліджувати результати.


### Задача 2
*Запропонувати рішення для ідентифікації та знаходження певних елементів інтерфейсу гри для подальшого аналізу. Серед елементів, які потрібно визначити, є поточний час гри, кількість гравців, які ще знаходяться в грі, кількість вбивств гравця, інформація про життя та броню, а також поточний набір зброї.

Проте, гра дозволяє змінювати розмір та положення інтерфейсу та змінює його в залежності від контексту гри (наприклад, коли гравець керує транспортним засобом).
*

Для рішення даної задачі очевидно, треба використовувати зетекцію, однам з найкращих та найрозповсюджених рішень зараз є моделі сімейства YOLO, вони одній з найшидших, та мають високу швидкість, та великйи вибір моделей за розміром. Для нашого випадку має бути достатньою модель типу S або навіть N, з [YOLOv8](https://github.com/ultralytics/ultralytics) або ж [YOLOv5](https://github.com/ultralytics/yolov5). Зміна розміру та положення інтерфейсу може бути вирішена за допомогою аугментації даних, або ж за допомогою fine-tuning. При достатній кількості тренувальних даних проблем з визначенням елементів інтерфейсу не має бути. Якщо ми знаємо в якому саме регіоні екрана маж знаходитись обʼєкт, можна використовувати *regions of interest*. Це допоможе пришвидшити інференс за рахунок меншої роздільної здатності кропу.



### Задача 3
Запропонувати алгоритм для автоматичного розпізнавання тексту на зображенні та кластеризації його за визначеними параметрами. Запропонувати бібліотеки та алгоритми для попередньої обробки зображень, такі як вирівнювання кольорів та видалення шуму тощо, а також способи перетворення зображення в текст. Подано декілька прикладів вирізаних зображень для аналізу. Нижче наведено декілька приладів вирізаних зображень:

Для задачі розпізнавання тексту можна використовувати класичні алгоритми OCR, такі як [PyTesseract](https://github.com/tesseract-ocr/tesseract), у звʼязці з OpenCV. PyTesseract вже включає в себе частину предобробки зображення. Але щоб правильно базово підготувати дані треба виконати наступні кроки: 
1. Перевести зображення в чорно-біле `cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)`
2. За допомогою блюра видалити шум `cv2.medianBlur(image,5)`
3. Трешхолд `cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]` - допоможе видалити шум та підвищити контрастність тексту
4. Ерозія `cv2.erode(image, kernel, iterations=1)`  - допоможе позбавитись шуму та розділити символи, що пересікаються.
5. Знайти контури `cv2.Canny(image, 100, 200)` - допоможе знайти контури тексту
6. Подати на вхід PyTesseract - `pytesseract.image_to_string(image)`

Це є базовий пайплайн з використанням PyTesseract. Процес очищення може бути розширений також методами dilation та іншими. Інструмент одразу кластеризує текст. Знаючи позицію тексту на зображенні, ми зможемо легко визначити його тип.

З подібних підходів також є [EasyOCR](https://github.com/JaidedAI/EasyOCR), але він потребує вже більше обчислювальних потужностей, та є більш повільним.






